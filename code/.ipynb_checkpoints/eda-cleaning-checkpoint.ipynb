{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to strip html off of body text\n",
    "def clean_data(review):\n",
    "    no_html = BeautifulSoup(review).text\n",
    "    lower_case = no_html.lower()\n",
    "    \n",
    "    return lower_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Follow me</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientific research should be ethical, not evil..</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Book recommendations for series similar to Leo...</td>\n",
       "      <td>Basically, a series where an individual or gr...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recommendations for fantasy that turns out to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scifi Horror recommendation</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                          Follow me   \n",
       "1  Scientific research should be ethical, not evil..   \n",
       "2  Book recommendations for series similar to Leo...   \n",
       "3  Recommendations for fantasy that turns out to ...   \n",
       "4                        Scifi Horror recommendation   \n",
       "\n",
       "                                            selftext subreddit  \n",
       "0                                                NaN     scifi  \n",
       "1                                                NaN     scifi  \n",
       "2   Basically, a series where an individual or gr...     scifi  \n",
       "3                                                NaN     scifi  \n",
       "4                                          [removed]     scifi  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "posts = pd.read_csv('../data/posts.csv', index_col=0)\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           0\n",
       "selftext     3373\n",
       "subreddit       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "posts.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for revoved posts\n",
    "len(posts[posts['selftext'] == '[removed]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts[posts['selftext'] == '[deleted]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN and [removed] from selftext as these won't be helpful\n",
    "posts_drop = posts.dropna()\n",
    "posts_drop = posts_drop[posts_drop['selftext'] != '[removed]']\n",
    "posts_drop = posts_drop[posts_drop['selftext'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "posts_drop.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Book recommendations for series similar to Leo...</td>\n",
       "      <td>Basically, a series where an individual or gr...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just finished *Other Space* (2015), the flag...</td>\n",
       "      <td>I am not a shill for Yahoo! I swear!\\n\\nThe se...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can I get some help once again with a book tit...</td>\n",
       "      <td>Okay this is getting rediculous but this is bu...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Book recommendations for series similar to Leo...   \n",
       "1  I just finished *Other Space* (2015), the flag...   \n",
       "2  Can I get some help once again with a book tit...   \n",
       "\n",
       "                                            selftext subreddit  \n",
       "0   Basically, a series where an individual or gr...     scifi  \n",
       "1  I am not a shill for Yahoo! I swear!\\n\\nThe se...     scifi  \n",
       "2  Okay this is getting rediculous but this is bu...     scifi  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_drop.drop(columns=['index'], inplace=True)\n",
    "posts_drop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fantasy    3820\n",
       "scifi       791\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many are left\n",
    "posts_drop['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not a shill for Yahoo! I swear!\\n\\nThe series is short (total binge time about as long as Peter Jackson\\'s extended *The Return of the King*), and ends on sort of a cliffhanger which means almost anything I say will be a spoiler, so please just watch and enjoy. I promise you will laugh. The best description I can give without spoiling anything is this:\\n\\nImagine the basic premise of *Space:1999* or *Star Trek: Voyager*, but with the Crew of cadets from the *Star Trek: Deep Space Nine* episode [\"Valiant\"](https://www.imdb.com/title/tt0708657/), all wrapped up in the gallows humour, social commentary, and prop comedy elements of *Red Dwarf*. Joel Hodgson is a mentoring presence among a cast of young actors who are clearly taking his advice about comedic delivery and timing to heart, as their jokes hit the mark more often than not. The series manages to give its own lightheartedly cynical take on a big batch of science fiction\\'s staple tropes, everything from cloned organ farming to time dilation fields to first contact procedures to robot revolutions.\\n\\nI cannot recommend this show highly enough.\\n\\nThese are all official Yahoo! links with limited or no commercials:\\n\\n- [Episode 1: Into the Great Beyond...Beyond](https://www.yahoo.com/entertainment/v/episode-1-great-beyond-beyond-070100400.html) \\n- [Episode 2: Getting to Know You](https://www.yahoo.com/entertainment/v/episode-2-getting-know-070100298.html)\\n- [Episode 3: The Death of A.R.T.](https://finance.yahoo.com/video/episode-3-death-art-070100167.html)\\n- [Episode 4: Ted Talks](https://www.yahoo.com/entertainment/video/episode-4-ted-talks-070100825.html)\\n- [Episode 5: Trouble\\'s Brewing](https://finance.yahoo.com/video/episode-5-troubles-brewing-070100427.html)\\n- [Episode 6: Powerless](https://finance.yahoo.com/video/episode-6-powerless-070100452.html)\\n- [Episode 7: First Contact](https://sg.style.yahoo.com/episode-7-first-contact-070100091.html)\\n- [Episode 8: Finale](https://news.yahoo.com/video/episode-8-finale-070100014.html)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for html within the text\n",
    "posts_drop['selftext'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/y-1XeDAjrLc\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/0dtuFd1SBKw\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/O_kYtM1nGJ8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/channel/UCQVzGcmBoDakdJv7JH-DYlg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/Z6HY7UmBmog\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://discord.gg/aGRPrXf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B08LW1R2SL\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.com/dp/B08QSM8BGP\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=b5lWYY9Tszc\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.com/Beyond-Horizon-Jacob-Rousseau-ebook/dp/B08PNZG2ZP/ref=sr_1_2?crid=J3N3SIN53J3U&amp;dchild=1&amp;keywords=beyond+the+horizon&amp;qid=1607090878&amp;sprefix=Beyond+the+hori%2Caps%2C188&amp;sr=8-2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=hAifdhM3i8c\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.ca/Emperors-Blades-Chronicle-Unhewn-Throne-ebook/dp/B00FCQQCX6/ref=gp_aw_ybh_a_1?_encoding=UTF8&amp;psc=1&amp;refRID=302S9XP7WMBDBRV25D11\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://twitter.com/nkjemisin/status/1344329346232840193?s=19\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# strip html\n",
    "posts_drop['clean_text'] = posts_drop['selftext'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text some more\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "\n",
    "# remove unnecessary characters\n",
    "posts_drop.replace({'\\n': ''}, regex=True, inplace=True)\n",
    "posts_drop.replace({'\\[': ''}, regex=True, inplace=True)\n",
    "posts_drop.replace({'\\]': ''}, regex=True, inplace=True)\n",
    "posts_drop.replace(r'\\\\', '', regex=True, inplace=True)\n",
    "posts_drop.replace(r'>', '', regex=True, inplace=True) #remove converted &gt; to > by BeautifulSoup\n",
    "\n",
    "# remove urls from text\n",
    "# https://stackoverflow.com/questions/56358888/how-to-remove-https-links-from-a-string-column-in-pandas\n",
    "posts_drop['clean_text'] = posts_drop['clean_text'].str.replace(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am not a shill for yahoo! i swear!the series is short (total binge time about as long as peter jackson\\'s extended *the return of the king*), and ends on sort of a cliffhanger which means almost anything i say will be a spoiler, so please just watch and enjoy. i promise you will laugh. the best description i can give without spoiling anything is this:imagine the basic premise of *space:1999* or *star trek: voyager*, but with the crew of cadets from the *star trek: deep space nine* episode \"valiant\"( all wrapped up in the gallows humour, social commentary, and prop comedy elements of *red dwarf*. joel hodgson is a mentoring presence among a cast of young actors who are clearly taking his advice about comedic delivery and timing to heart, as their jokes hit the mark more often than not. the series manages to give its own lightheartedly cynical take on a big batch of science fiction\\'s staple tropes, everything from cloned organ farming to time dilation fields to first contact procedures to robot revolutions.i cannot recommend this show highly enough.these are all official yahoo! links with limited or no commercials:- episode 1: into the great beyond...beyond( - episode 2: getting to know you( episode 3: the death of a.r.t.( episode 4: ted talks( episode 5: trouble\\'s brewing( episode 6: powerless( episode 7: first contact( episode 8: finale('"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for html and lowercase\n",
    "posts_drop['clean_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Book recommendations for series similar to Leo...</td>\n",
       "      <td>Basically, a series where an individual or gr...</td>\n",
       "      <td>scifi</td>\n",
       "      <td>basically, a series where an individual or gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just finished *Other Space* (2015), the flag...</td>\n",
       "      <td>I am not a shill for Yahoo! I swear!The series...</td>\n",
       "      <td>scifi</td>\n",
       "      <td>i am not a shill for yahoo! i swear!the series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can I get some help once again with a book tit...</td>\n",
       "      <td>Okay this is getting rediculous but this is bu...</td>\n",
       "      <td>scifi</td>\n",
       "      <td>okay this is getting rediculous but this is bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Book recommendations for series similar to Leo...   \n",
       "1  I just finished *Other Space* (2015), the flag...   \n",
       "2  Can I get some help once again with a book tit...   \n",
       "\n",
       "                                            selftext subreddit  \\\n",
       "0   Basically, a series where an individual or gr...     scifi   \n",
       "1  I am not a shill for Yahoo! I swear!The series...     scifi   \n",
       "2  Okay this is getting rediculous but this is bu...     scifi   \n",
       "\n",
       "                                          clean_text  \n",
       "0  basically, a series where an individual or gro...  \n",
       "1  i am not a shill for yahoo! i swear!the series...  \n",
       "2  okay this is getting rediculous but this is bu...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_drop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4410"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct values for clean text\n",
    "posts_drop['clean_text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "posts_drop.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fantasy    0.828559\n",
       "scifi      0.171441\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_drop['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe based on selftext ended up very unbalanced, so I will not be using posts selftext for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean posts\n",
    "posts_drop.to_csv('../data/posts_clean_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9596"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct values for titles\n",
    "posts['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9727, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates from title\n",
    "posts_title = posts.drop_duplicates()\n",
    "posts_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fantasy    4981\n",
       "scifi      4746\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_title['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean titles\n",
    "posts_title.to_csv('../data/posts_clean_title.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe based on titles is more balanced than based on selftext, but let's see what comments can give us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use subtitles</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For neither ever, nor never</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It reminds me of Fringe too</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw S01 in English, had to resort to subtitles...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When season 2 was released, all the recaps I h...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body subreddit\n",
       "0                                      use subtitles     scifi\n",
       "1                        For neither ever, nor never     scifi\n",
       "2                        It reminds me of Fringe too     scifi\n",
       "3  Saw S01 in English, had to resort to subtitles...     scifi\n",
       "4  When season 2 was released, all the recaps I h...     scifi"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "comments = pd.read_csv('../data/comments.csv', index_col=0)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body         0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "comments.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for removed posts\n",
    "len(comments[comments['body'] == '[removed]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments[comments['body'] == '[deleted]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use subtitles</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For neither ever, nor never</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It reminds me of Fringe too</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw S01 in English, had to resort to subtitles...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When season 2 was released, all the recaps I h...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Didn't Orson Scott Card donate his money to an...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>:( I liked the first.  The second was weaker b...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Lmao he is a bit of a chad honestly\\n\\nYA: You...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>It's not at all terrible, it's really goddamn ...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>I have not read any of the others on your list...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body subreddit\n",
       "0                                         use subtitles     scifi\n",
       "1                           For neither ever, nor never     scifi\n",
       "2                           It reminds me of Fringe too     scifi\n",
       "3     Saw S01 in English, had to resort to subtitles...     scifi\n",
       "4     When season 2 was released, all the recaps I h...     scifi\n",
       "...                                                 ...       ...\n",
       "9995  Didn't Orson Scott Card donate his money to an...   Fantasy\n",
       "9996  :( I liked the first.  The second was weaker b...   Fantasy\n",
       "9997  Lmao he is a bit of a chad honestly\\n\\nYA: You...   Fantasy\n",
       "9998  It's not at all terrible, it's really goddamn ...   Fantasy\n",
       "9999  I have not read any of the others on your list...   Fantasy\n",
       "\n",
       "[9935 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since there are no NaNs, drop [removed] from body as these won't be helpful\n",
    "comments_drop = comments[comments['body'] != '[removed]']\n",
    "comments_drop = comments[comments['body'] != '[deleted]']\n",
    "comments_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>9995</td>\n",
       "      <td>Didn't Orson Scott Card donate his money to an...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9931</th>\n",
       "      <td>9996</td>\n",
       "      <td>:( I liked the first.  The second was weaker b...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>9997</td>\n",
       "      <td>Lmao he is a bit of a chad honestly\\n\\nYA: You...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>9998</td>\n",
       "      <td>It's not at all terrible, it's really goddamn ...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>9999</td>\n",
       "      <td>I have not read any of the others on your list...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               body subreddit\n",
       "9930   9995  Didn't Orson Scott Card donate his money to an...   Fantasy\n",
       "9931   9996  :( I liked the first.  The second was weaker b...   Fantasy\n",
       "9932   9997  Lmao he is a bit of a chad honestly\\n\\nYA: You...   Fantasy\n",
       "9933   9998  It's not at all terrible, it's really goddamn ...   Fantasy\n",
       "9934   9999  I have not read any of the others on your list...   Fantasy"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset index and drop old index column\n",
    "comments_drop.reset_index(inplace=True)\n",
    "comments_drop.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use subtitles</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For neither ever, nor never</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It reminds me of Fringe too</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw S01 in English, had to resort to subtitles...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When season 2 was released, all the recaps I h...</td>\n",
       "      <td>scifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>Didn't Orson Scott Card donate his money to an...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9931</th>\n",
       "      <td>:( I liked the first.  The second was weaker b...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9932</th>\n",
       "      <td>Lmao he is a bit of a chad honestly\\n\\nYA: You...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>It's not at all terrible, it's really goddamn ...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9934</th>\n",
       "      <td>I have not read any of the others on your list...</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9935 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body subreddit\n",
       "0                                         use subtitles     scifi\n",
       "1                           For neither ever, nor never     scifi\n",
       "2                           It reminds me of Fringe too     scifi\n",
       "3     Saw S01 in English, had to resort to subtitles...     scifi\n",
       "4     When season 2 was released, all the recaps I h...     scifi\n",
       "...                                                 ...       ...\n",
       "9930  Didn't Orson Scott Card donate his money to an...   Fantasy\n",
       "9931  :( I liked the first.  The second was weaker b...   Fantasy\n",
       "9932  Lmao he is a bit of a chad honestly\\n\\nYA: You...   Fantasy\n",
       "9933  It's not at all terrible, it's really goddamn ...   Fantasy\n",
       "9934  I have not read any of the others on your list...   Fantasy\n",
       "\n",
       "[9935 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop old index\n",
    "comments_drop.drop(columns=['index'], inplace=True)\n",
    "comments_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scifi      4972\n",
       "Fantasy    4963\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many are left\n",
    "comments_drop['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scifi      0.500453\n",
       "Fantasy    0.499547\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_drop['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset based on comments is balanced, so lets do further cleaning. I will be using comments for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Dark Matter**.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for html within the text\n",
    "comments_drop['body'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=66uduSIp-3k\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://m.youtube.com/watch?v=d6pHR2sK3L0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://gizmoblaze.com/2020/05/22/patrick-rothfuss-the-doors-of-stone-release-date-confirmed-by-amazon-august-2020-kingkiller-chronicles-final-book-is-here/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.com/Schismatrix-Plus-Complete-Shapers-Mechanists-Universe/dp/0441003702\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://en.wikipedia.org/wiki/Sturgeon's_law\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://tvtropes.org/pmwiki/pmwiki.php/Main/PlanetLooters\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/leCMTfjCEFM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.imgur.com/M1UlXsY.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.imdb.com/title/tt0080684/releaseinfo?ref_=tt_ov_inf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.imgur.com/kWnkLRD.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://gfycat.com/flowerywickedgarpike\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://en.m.wikipedia.org/wiki/Joe_Abercrombie\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.kickstarter.com/projects/harryconnolly/the-iron-gate-break-kickstarter/posts/2966993?ref=ksr_email_backer_project_update_registered_users\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.pinimg.com/originals/5c/44/a9/5c44a984fef2f369360b159fa457c836.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=b-9v7ecdOsg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "<ipython-input-37-6ceb8f90ef0f>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_drop['clean_body'] = comments_drop['body'].apply(clean_data)\n"
     ]
    }
   ],
   "source": [
    "# apply finction to strip html\n",
    "comments_drop['clean_body'] = comments_drop['body'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "<ipython-input-38-d675d7b3f33e>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_drop['clean_body'] = comments_drop['clean_body'].str.replace(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', \"\")\n"
     ]
    }
   ],
   "source": [
    "# clean text some more\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "\n",
    "# remove unnecessary characters\n",
    "comments_drop.replace({'\\n': ''}, regex=True, inplace=True)\n",
    "comments_drop.replace({'\\[': ''}, regex=True, inplace=True)\n",
    "comments_drop.replace({'\\]': ''}, regex=True, inplace=True)\n",
    "comments_drop.replace({'@': ''}, regex=True, inplace=True)\n",
    "comments_drop.replace({'\\*': ''}, regex=True, inplace=True)\n",
    "comments_drop.replace(r'\\\\', '', regex=True, inplace=True)\n",
    "comments_drop.replace(r'>', '', regex=True, inplace=True) #remove converted &gt; to > by BeautifulSoup\n",
    "\n",
    "# remove urls from text\n",
    "# https://stackoverflow.com/questions/56358888/how-to-remove-https-links-from-a-string-column-in-pandas\n",
    "comments_drop['clean_body'] = comments_drop['clean_body'].str.replace(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dark matter.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for html, lowercase, and other unnecessary characters\n",
    "comments_drop['clean_body'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"do androids dream of electric sheep? - philip k. dickit's a detective set in the future and the movie bladerunner  was based on it.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_drop['clean_body'][4401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9612"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct values for clean body text\n",
    "comments_drop['clean_body'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-44-0903f7e89fd4>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_drop.drop_duplicates(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9652, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates from body text\n",
    "comments_drop.drop_duplicates(inplace=True)\n",
    "comments_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-0d248ab5a2e3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_drop['target'] = comments['subreddit'].map({'scifi': 1, 'Fantasy': 0})\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use subtitles</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for neither ever, nor never</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it reminds me of fringe too</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saw s01 in english, had to resort to subtitles...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when season 2 was released, all the recaps i h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_body  target\n",
       "0                                      use subtitles       1\n",
       "1                        for neither ever, nor never       1\n",
       "2                        it reminds me of fringe too       1\n",
       "3  saw s01 in english, had to resort to subtitles...       1\n",
       "4  when season 2 was released, all the recaps i h...       1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create target column iwth 1 being scifi and 0 - fantasy\n",
    "comments_drop['target'] = comments['subreddit'].map({'scifi': 1, 'Fantasy': 0})\n",
    "comments_drop.drop(columns=['subreddit', 'body'], inplace=True)\n",
    "comments_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500622\n",
       "1    0.499378\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_drop['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean titles\n",
    "comments_drop.to_csv('../data/comments_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posts data is very unbalanced, with fantasy having 7477 entries and scifi having 2596 entried, I wont be using selftext of a post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return ' '.join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-a4f59282ddbc>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comments_drop['lemma'] = comments_drop['clean_body'].apply(lemmatize)\n"
     ]
    }
   ],
   "source": [
    "comments_drop['lemma'] = comments_drop['clean_body'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_body</th>\n",
       "      <th>target</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use subtitles</td>\n",
       "      <td>1</td>\n",
       "      <td>use subtitle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for neither ever, nor never</td>\n",
       "      <td>1</td>\n",
       "      <td>for neither ever , nor never</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    clean_body  target                         lemma\n",
       "0                use subtitles       1                  use subtitle\n",
       "1  for neither ever, nor never       1  for neither ever , nor never"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_drop.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_drop.to_csv('../data/comments_lemma.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Most common words in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_comments = pd.read_csv('../data/scifi_comments_10000.csv', index_col=0)\n",
    "fantasy_comments = pd.read_csv('../data/fantasy_comments_10000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=66uduSIp-3k\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://m.youtube.com/watch?v=d6pHR2sK3L0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://gizmoblaze.com/2020/05/22/patrick-rothfuss-the-doors-of-stone-release-date-confirmed-by-amazon-august-2020-kingkiller-chronicles-final-book-is-here/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.amazon.com/Schismatrix-Plus-Complete-Shapers-Mechanists-Universe/dp/0441003702\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://en.wikipedia.org/wiki/Sturgeon's_law\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://tvtropes.org/pmwiki/pmwiki.php/Main/PlanetLooters\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/leCMTfjCEFM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.imgur.com/M1UlXsY.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.imdb.com/title/tt0080684/releaseinfo?ref_=tt_ov_inf\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.imgur.com/kWnkLRD.gif\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://gfycat.com/flowerywickedgarpike\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.imgflip.com/3034ny.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://en.m.wikipedia.org/wiki/Spaceship_Earth\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://youtu.be/5d0fcu6JXxY\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://vimeo.com/391434825\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://img.fantasticfiction.com/images/n0/n1835.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=6rwcNv3dhTM\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/user/JaiWbio\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.nature.com/articles/436602a\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://archive.org/details/neutronstar00larr/mode/2up\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# repeat same process to clean text for common words in scifi\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "scifi = scifi_comments[scifi_comments['body'] != '[removed]']\n",
    "scifi = scifi[scifi['body'] != '[deleted]']\n",
    "\n",
    "scifi.reset_index(inplace=True)\n",
    "scifi.drop(columns=['index'], inplace=True)\n",
    "scifi['clean_body'] = scifi['body'].apply(clean_data)\n",
    "scifi.replace({'\\n': ''}, regex=True, inplace=True)\n",
    "scifi.replace({'\\[': ''}, regex=True, inplace=True)\n",
    "scifi.replace({'\\]': ''}, regex=True, inplace=True)\n",
    "scifi.replace({'@': ''}, regex=True, inplace=True)\n",
    "scifi.replace({'\\*': ''}, regex=True, inplace=True)\n",
    "scifi.replace(r'\\\\', '', regex=True, inplace=True)\n",
    "scifi.replace(r'>', '', regex=True, inplace=True) #remove converted &gt; to > by BeautifulSoup\n",
    "\n",
    "# remove URLs\n",
    "# https://stackoverflow.com/questions/56358888/how-to-remove-https-links-from-a-string-column-in-pandas\n",
    "scifi['clean_body'] = scifi['clean_body'].str.replace(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-90-b6af04b75384>:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  fantasy = fantasy[scifi['body'] != '[deleted]']\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://en.m.wikipedia.org/wiki/Joe_Abercrombie\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.kickstarter.com/projects/harryconnolly/the-iron-gate-break-kickstarter/posts/2966993?ref=ksr_email_backer_project_update_registered_users\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://i.pinimg.com/originals/5c/44/a9/5c44a984fef2f369360b159fa457c836.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=b-9v7ecdOsg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://tvtropes.org/pmwiki/pmwiki.php/ArtifactTitle/Literature\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.brandonsanderson.com/brandon-sanderson-online-library/\n",
      "\n",
      "https://www.brandonsanderson.com/warbreaker-rights-and-downloads/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://www.reddit.com/r/Fantasy/comments/iwwqj3/if_fantasy_authors_covered_novels_the_way/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/yuli/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://openlibrary.org/subjects/fantasy#sort=date_published&amp;ebooks=true\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# repeat same process to clean text for common words in fantasy\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
    "fantasy = fantasy_comments[fantasy_comments['body'] != '[removed]']\n",
    "fantasy = fantasy[scifi['body'] != '[deleted]']\n",
    "\n",
    "fantasy.reset_index(inplace=True)\n",
    "fantasy.drop(columns=['index'], inplace=True)\n",
    "fantasy['clean_body'] = fantasy['body'].apply(clean_data)\n",
    "fantasy.replace({'\\n': ''}, regex=True, inplace=True)\n",
    "fantasy.replace({'\\[': ''}, regex=True, inplace=True)\n",
    "fantasy.replace({'\\]': ''}, regex=True, inplace=True)\n",
    "fantasy.replace({'@': ''}, regex=True, inplace=True)\n",
    "fantasy.replace({'\\*': ''}, regex=True, inplace=True)\n",
    "fantasy.replace(r'\\\\', '', regex=True, inplace=True)\n",
    "fantasy.replace(r'>', '', regex=True, inplace=True) #remove converted &gt; to > by BeautifulSoup\n",
    "\n",
    "# remove URLs\n",
    "# https://stackoverflow.com/questions/56358888/how-to-remove-https-links-from-a-string-column-in-pandas\n",
    "fantasy['clean_body'] = fantasy['clean_body'].str.replace(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>associated_award</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>...</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>treatment_tags</th>\n",
       "      <th>edited</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>clean_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>CheatMaple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>scifi</td>\n",
       "      <td>t5_2qh2z</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>use subtitles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>CheatMaple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>text</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>scifi</td>\n",
       "      <td>t5_2qh2z</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>for neither ever, nor never</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  associated_award      author  author_flair_background_color  \\\n",
       "0                             NaN  CheatMaple                            NaN   \n",
       "1                             NaN  CheatMaple                            NaN   \n",
       "\n",
       "   author_flair_css_class author_flair_richtext  author_flair_template_id  \\\n",
       "0                     NaN                                             NaN   \n",
       "1                     NaN                                             NaN   \n",
       "\n",
       "   author_flair_text  author_flair_text_color author_flair_type  ...  \\\n",
       "0                NaN                      NaN              text  ...   \n",
       "1                NaN                      NaN              text  ...   \n",
       "\n",
       "  send_replies  stickied  subreddit subreddit_id total_awards_received  \\\n",
       "0         True     False      scifi     t5_2qh2z                     0   \n",
       "1         True     False      scifi     t5_2qh2z                     0   \n",
       "\n",
       "   treatment_tags  edited author_cakeday distinguished  \\\n",
       "0                     NaN            NaN           NaN   \n",
       "1                     NaN            NaN           NaN   \n",
       "\n",
       "                    clean_body  \n",
       "0                use subtitles  \n",
       "1  for neither ever, nor never  \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scifi.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like      1737\n",
       "just      1589\n",
       "good      1071\n",
       "really     972\n",
       "series     967\n",
       "think      913\n",
       "time       897\n",
       "don        852\n",
       "star       822\n",
       "read       802\n",
       "book       787\n",
       "sci        671\n",
       "people     666\n",
       "fi         660\n",
       "movie      660\n",
       "books      617\n",
       "trek       609\n",
       "space      585\n",
       "story      583\n",
       "great      570\n",
       "dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify most common words in scifi\n",
    "X_sci = scifi['clean_body']\n",
    "\n",
    "cvect_sci = CountVectorizer(stop_words='english')\n",
    "dtm_sci = cvect_sci.fit_transform(X_sci)\n",
    "\n",
    "pd.DataFrame(\n",
    "    dtm_sci.toarray(), \n",
    "    columns=cvect_sci.get_feature_names()\n",
    ").sum().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book          2330\n",
       "like          2163\n",
       "read          2008\n",
       "books         1997\n",
       "series        1993\n",
       "just          1853\n",
       "fantasy       1615\n",
       "really        1460\n",
       "think         1392\n",
       "good          1150\n",
       "people        1050\n",
       "don           1008\n",
       "time           978\n",
       "ve             947\n",
       "characters     837\n",
       "story          798\n",
       "world          777\n",
       "lot            739\n",
       "reading        688\n",
       "author         679\n",
       "way            655\n",
       "great          653\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify most common words in fantasy\n",
    "X_fan = fantasy['clean_body']\n",
    "\n",
    "cvect_fan = CountVectorizer(stop_words='english')\n",
    "dtm_fan = cvect_fan.fit_transform(X_fan)\n",
    "\n",
    "pd.DataFrame(\n",
    "    dtm_fan.toarray(), \n",
    "    columns=cvect_fan.get_feature_names()\n",
    ").sum().sort_values(ascending=False)[:22]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
